{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "61e94338-82e2-4ec3-9fbb-06a14d998199",
      "metadata": {
        "id": "61e94338-82e2-4ec3-9fbb-06a14d998199"
      },
      "source": [
        "### Fitting a simple model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1c962d6-9833-4c3a-a329-01773bc94320",
      "metadata": {
        "id": "c1c962d6-9833-4c3a-a329-01773bc94320"
      },
      "source": [
        "Notes:\n",
        "\n",
        "As in tutorial 1, we must start with importing useful data science libraries. We will also be using the library `sci-kit learn` as part of this tutorial. We will also use some functions from the sci-kit learn package during this tutorial, but since we don't want to import the whole library we will do that later in the code.\n",
        "\n",
        "func-ai -> common imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ac8eaec-427a-4614-907d-66ffbada8707",
      "metadata": {
        "id": "2ac8eaec-427a-4614-907d-66ffbada8707"
      },
      "outputs": [],
      "source": [
        "## Open func-ai, and click on common imports. Copy the code here and then execute the cell.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy as sp\n",
        "import seaborn as sns\n",
        "import matplotlib   ### note to func-ai --> this is probably unnecessary\n",
        "from matplotlib import pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25c380d5-bd9e-4ce9-89f2-63ea7f21a24c",
      "metadata": {
        "id": "25c380d5-bd9e-4ce9-89f2-63ea7f21a24c"
      },
      "source": [
        "Note: Again, we will load the [adult](https://archive.ics.uci.edu/dataset/2/adult) dataset, an open source dataset based on a sample of US census data collected in 1994.\n",
        "\n",
        "We name the dataset `data`, and use the pandas `read_csv` function to load this dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tVeGcei2wFAO",
        "outputId": "693702a6-1736-4a49-bff7-4e08d82086f4"
      },
      "id": "tVeGcei2wFAO",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b8edd42-57a3-4c6b-9ff3-f88e613cd221",
      "metadata": {
        "id": "0b8edd42-57a3-4c6b-9ff3-f88e613cd221"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/drive/MyDrive/SampleData/adult.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d80149c5-1823-4283-a9be-9a860268b018",
      "metadata": {
        "id": "d80149c5-1823-4283-a9be-9a860268b018"
      },
      "source": [
        "Note --- First we will look at a small sample of the data.\n",
        "\n",
        "You can try and do this yourself, it's the same as tutorial 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e0f0dbd-4bd4-4f7c-8e1d-b1a3bb3c7229",
      "metadata": {
        "id": "1e0f0dbd-4bd4-4f7c-8e1d-b1a3bb3c7229"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "64949dab-2ee5-4040-936c-7a8f61249af1",
      "metadata": {
        "id": "64949dab-2ee5-4040-936c-7a8f61249af1"
      },
      "source": [
        "We'll also check the column names again."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a742b614-0478-4c41-8b30-6a413a70ad43",
      "metadata": {
        "id": "a742b614-0478-4c41-8b30-6a413a70ad43"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "12ddaadd-5815-48d0-b792-a39624344e91",
      "metadata": {
        "id": "12ddaadd-5815-48d0-b792-a39624344e91"
      },
      "source": [
        "We see that the table has 15 columns, each column having a name such as 'age', 'workclass', etc. Further information about the data can be found on [this website](https://archive.ics.uci.edu/dataset/2/adult). It is important to notice that some columns contain numeric data, others contain text. Later we will see how we can use data science techniques with text data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71975182-dca9-4542-a5dc-5b7453961450",
      "metadata": {
        "id": "71975182-dca9-4542-a5dc-5b7453961450"
      },
      "source": [
        "Let's look at the data type for each column.\n",
        "\n",
        "func-ai --> Exploratory Data Analysis --> Data Type\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "70b19795-6142-4df6-bd0c-ecff5c9e7e15",
      "metadata": {
        "id": "70b19795-6142-4df6-bd0c-ecff5c9e7e15"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "ed793516-a48f-4f09-9a41-8942687b4720",
      "metadata": {
        "id": "ed793516-a48f-4f09-9a41-8942687b4720"
      },
      "source": [
        "In this tutorial we will try and use dataset features to predict whether each person earns more or less than \\$50k per year - this is recorded in the `class` column. We will call this our \"target\" column, and you may also see it referred to as \"y\". This comes from a mathematical perspective, in machine learning tasks we are trying to find a function $f$ which accurately maps our input data $X$ to an output column $y$).\n",
        "\n",
        "You will notice that most of the columns in this dataset are not numerical. For our first model, we will only use the numeric columns as these can be directly used with our modelling functions. Later we will see how to convert non-numeric columns in to something which can be used by a model."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6c269606-943c-43dc-b1e6-4842f0f909b3",
      "metadata": {
        "id": "6c269606-943c-43dc-b1e6-4842f0f909b3"
      },
      "source": [
        "First we can look to see if there are any features that the target has a dependent relationship with. We can do this using Chi-squared test, but there are also many other approaches - see [here](https://scikit-learn.org/stable/modules/feature_selection.html).\n",
        "\n",
        "func-ai -> menu -> exploratory data analysis -> feature importance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "id": "FYOyhUYoY990"
      },
      "id": "FYOyhUYoY990",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80850688-4c53-4838-9fea-4cb01a3f35a4",
      "metadata": {
        "id": "80850688-4c53-4838-9fea-4cb01a3f35a4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d7f05ccf-aa12-4b11-96bc-8047479238c2",
      "metadata": {
        "id": "d7f05ccf-aa12-4b11-96bc-8047479238c2"
      },
      "source": [
        "We can see that capital-gain and capital-loss are the most important features. We'll use a Random Forest classifier to try and predict the income class of each person using these two features.\n",
        "\n",
        "In order to measure the predictive performance of a model, we set aside a random sample from the dataset before training the model. This is called a 'holdout' split, or a 'test' split. Doing this means that the model will be tested on data it has not been directly trained on, which gives us a more accurate indication of its performance on unseen data. A typical size for this would be 20-30% of data, depending on the size of your total dataset.\n",
        "\n",
        "We will measure the performance using a [classification report](https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report), which calculates the precision, recall and f1 score for each class, and a [confusion matrix](https://scikit-learn.org/stable/modules/model_evaluation.html#confusion-matrix).\n",
        "\n",
        "A Random Forest Classifier is an ensemble of Decision Tree Classifiers, and func-ai provides code to plot one of these decision trees to give insight in to the decision rules used to split the dataset in to groups which are associated with one of the classes in the target.\n",
        "\n",
        "func-ai -> modelling -> random forests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b00cdd57-c39c-4957-9783-5288a5c8f467",
      "metadata": {
        "id": "b00cdd57-c39c-4957-9783-5288a5c8f467"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2449a128-78c8-4df4-a2e8-66b9617fcf83",
      "metadata": {
        "id": "2449a128-78c8-4df4-a2e8-66b9617fcf83"
      },
      "source": [
        "#### Exercises:\n",
        "1. Train a random forest classifier on the features age, hours-per-week and education-num. How does the performance differ to the model we trained?\n",
        "\n",
        "2. `sci-kit learn` contains many different classification models, and the power of the library is that all of the models follow the same input-output structure (known as an API). This means that all we have to do to train a different model is use a different model name. You can try this yourself:\n",
        "\n",
        "   - Make a copy of the modelling code for the Random Forest in a cell below. Do not include the code to plot a decision tree.\n",
        "   - Replace the line `from sklearn.ensemble import RandomForestClassifier` with `from sklearn.neighbors import KNeighborsClassifier`\n",
        "   - Replace the line `model = RandomForestClassifier(...)` with `model = KNeighborsClassifier()` and rerun the code.\n",
        "   - Compare the Neighbors model with the RandomForest. Then compare the performance if you use the age, hours-per-week and education-num instead of capital-gain and capital-loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1467d77-051f-4bd1-980e-89d40dd5838f",
      "metadata": {
        "jupyter": {
          "source_hidden": true
        },
        "scrolled": true,
        "id": "f1467d77-051f-4bd1-980e-89d40dd5838f"
      },
      "outputs": [],
      "source": [
        "### Answer for 1\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "\n",
        "column_names = ['age','hours-per-week','education-num']\n",
        "X = data.dropna()[column_names]\n",
        "y = data.dropna()['class']   ###Â class is a protected name so doing .class here breaks\n",
        "\n",
        "# FUNC TIP:  if you dont want to optimise for single class, replace the line above with following one\n",
        "# y = array_name['target_feature']\n",
        "\n",
        "### i think it should be made clear that in the multi-class case,\n",
        "### 'optimise for one class' actually means 'predict the presence of a single class'\n",
        "\n",
        "\n",
        "### Create the train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
        "\n",
        "### Fit the model to the data\n",
        "model = RandomForestClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "## calculate the performance scores on the test set\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(cf_matrix)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,5))  ## i think 5,5, is big enough\n",
        "sns.heatmap(cf_matrix, cmap=\"Reds\", fmt='.0f', ax=ax)\n",
        "\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.set_xlabel('\\n Predicted Outcome')\n",
        "ax.set_ylabel('Actual Outcome')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02502519-f7ca-4de1-a9cb-f3e25ca39691",
      "metadata": {
        "id": "02502519-f7ca-4de1-a9cb-f3e25ca39691"
      },
      "outputs": [],
      "source": [
        "### Answer for 2\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "\n",
        "column_names = ['age','hours-per-week','education-num']\n",
        "X = data.dropna()[column_names]\n",
        "y = data.dropna()['class']\n",
        "\n",
        "\n",
        "### Create the train and test datasets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
        "\n",
        "### Fit the model to the data\n",
        "model = KNeighborsClassifier()\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "## calculate the performance scores on the test set\n",
        "accuracy_score(y_test, y_pred)\n",
        "print(classification_report(y_test, y_pred))\n",
        "cf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(cf_matrix)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5,5))  ## i think 5,5, is big enough\n",
        "sns.heatmap(cf_matrix, cmap=\"Reds\", fmt='.0f', ax=ax)\n",
        "\n",
        "ax.set_title('Confusion Matrix');\n",
        "ax.set_xlabel('\\n Predicted Outcome')\n",
        "ax.set_ylabel('Actual Outcome')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48e03171-26cb-4cf2-a564-2f5bfba6046a",
      "metadata": {
        "id": "48e03171-26cb-4cf2-a564-2f5bfba6046a"
      },
      "source": [
        "## Fit a model to categorical columns\n",
        "\n",
        "Sci-kit learn models require numerical inputs, but our dataset has a number of categorical columns. One way to handle these is to use an 'encoder' which creates a mapping from categories to numbers. In func-ai, we have included a pre-model encoder which can handle different types of categorical data and convert them to an appropriate numerical format.\n",
        "\n",
        "func-ai -> data-wrangling -> PIPELINES Auto Encoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iYFeYEhvZxea"
      },
      "id": "iYFeYEhvZxea",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "162dbd4d-74c9-4e11-91e0-96e02176c6c6",
      "metadata": {
        "id": "162dbd4d-74c9-4e11-91e0-96e02176c6c6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a6296008-13a7-4053-a559-1d90f55ea706",
      "metadata": {
        "id": "a6296008-13a7-4053-a559-1d90f55ea706"
      },
      "source": [
        "Note: The preprocessed data has many more columns than our original dataset, because it uses [one-hot encoding](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.OneHotEncoder.html) to convert categorical columns. In order to use the modelling pipeline, you will need to run the preprocessor on the training data - the variable called `X_train` in the modelling cells above. The preprocessor actually has two methods - `fit_transform`, which adapts the preprocessor to a dataset and then applies the preprocessing functions, and `transform`, which uses an adapted preprocessor and applies it to a new dataset. This is important because we should only `fit` to our training data, and use `transform` on test data, because this gives us a realistic measure of model performance.\n",
        "\n",
        "To do this, create the modelling pipeline using func-ai -> modelling -> forests, and add the following lines (the first one is already provided by func:\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Then we use this processed data in our modelling pipeline as before, and you can compare the results with the other models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8955902-b3d4-4330-b5b3-6c0968834996",
      "metadata": {
        "id": "b8955902-b3d4-4330-b5b3-6c0968834996"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a7ae61ab-b623-4b9f-8e7f-a5d42e04051b",
      "metadata": {
        "id": "a7ae61ab-b623-4b9f-8e7f-a5d42e04051b"
      },
      "source": [
        "Exercise: Now that we have more features, we could explore tuning the random forest further to make the most of them. Try increasing the \"max_depth\" feature of the Random Forest to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f9752cd-aec4-4ab1-a05d-9763c23d98d2",
      "metadata": {
        "id": "4f9752cd-aec4-4ab1-a05d-9763c23d98d2"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e4231ae-98fe-427f-ac88-8241a1c7bebb",
      "metadata": {
        "id": "7e4231ae-98fe-427f-ac88-8241a1c7bebb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5445d4e8-8a8e-4b16-8281-e0393b253b27",
      "metadata": {
        "id": "5445d4e8-8a8e-4b16-8281-e0393b253b27"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f25a7f-7675-4030-8e7c-c33cdfc2eb66",
      "metadata": {
        "id": "d9f25a7f-7675-4030-8e7c-c33cdfc2eb66"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}